---
layout: post
title: "face parsing"
date:   2022-01-13
tags: [papers]
comments: false
author: houyaoqiqiqi
---
关于face parsing的综述

<!-- more -->

## basemodel
Rethink Dilated Convolution for Real-time Semantic Segmentation					 				
Polarized Self-Attention: Towards High-quality Pixel-wise Regression 
解读：[https://zhuanlan.zhihu.com/p/388716540](https://zhuanlan.zhihu.com/p/388716540)
[https://zhuanlan.zhihu.com/p/392148142](https://zhuanlan.zhihu.com/p/392148142)
## Pyramid Scene Parsing Network
PPM（Pyramid Pooling Module）模块
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634804777188-9ee11e0a-bcb9-4bab-99de-0bcd77616d7d.png#align=left&display=inline&height=399&id=u82e4aad2&margin=%5Bobject%20Object%5D&name=image.png&originHeight=798&originWidth=1800&size=714252&status=done&style=none&width=900)
## Deeplab v1
## BiSenet
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634817397910-20ca9c2d-7fe8-4a2b-ace4-fc0340758e99.png#align=left&display=inline&height=332&id=u784ff0bc&margin=%5Bobject%20Object%5D&name=image.png&originHeight=663&originWidth=969&size=338588&status=done&style=none&width=484.5)			 										 					
## ACFNet: Attentional Class Feature Network for Semantic Segmentation 
## ![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634887177343-444520d2-d664-4ea6-ba39-0eba3b4d4c73.png#align=left&display=inline&height=473&id=u4feb1a0a&margin=%5Bobject%20Object%5D&name=image.png&originHeight=946&originWidth=2044&size=636795&status=done&style=none&width=1022)
## Asymmetric Non-local Neural Networks for Semantic Segmentation 
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634887040114-824d13d6-f3ce-420e-8db6-9ae92ac189b2.png#align=left&display=inline&height=477&id=uc50d0704&margin=%5Bobject%20Object%5D&name=image.png&originHeight=954&originWidth=2124&size=1065048&status=done&style=none&width=1062)
# 							 					
## HRNet: Deep High-Resolution Representation Learning for Visual Recognition
[https://zhuanlan.zhihu.com/p/145465507](https://zhuanlan.zhihu.com/p/145465507)
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1635419736183-2a231ed2-7e35-4a90-bad1-4529cc95d434.png#align=left&display=inline&height=240&id=u94871140&margin=%5Bobject%20Object%5D&name=image.png&originHeight=481&originWidth=1440&size=495868&status=done&style=none&width=720)
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1635419749319-60b039c1-ba70-420f-9a10-0978fcd90c1d.png#align=left&display=inline&height=285&id=udef7da69&margin=%5Bobject%20Object%5D&name=image.png&originHeight=569&originWidth=1440&size=336146&status=done&style=none&width=720)
## Segmentation Transformer: Object-Contextual Representations for Semantic Segmentation
[https://zhuanlan.zhihu.com/p/145465507](https://zhuanlan.zhihu.com/p/145465507)
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1635419652695-285de2b6-defb-4aa4-af57-a41156908abc.png#align=left&display=inline&height=378&id=u86ca013a&margin=%5Bobject%20Object%5D&name=image.png&originHeight=755&originWidth=1440&size=545067&status=done&style=none&width=720)
## BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation 
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634885044848-331b9682-4a56-4cfd-af90-d8de8cf6a6af.png#align=left&display=inline&height=557&id=u589fcaf7&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1114&originWidth=1848&size=1217378&status=done&style=none&width=924)		 					
## Context Prior for Scene Segmentation 
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634884756751-39af91b5-08c0-48a7-8fc7-c51c953819ba.png#align=left&display=inline&height=477&id=u41e1fcc0&margin=%5Bobject%20Object%5D&name=image.png&originHeight=954&originWidth=1818&size=560641&status=done&style=none&width=909)					 					
					 					
## Dual Super-Resolution Learning for Semantic Segmentation 
[https://blog.csdn.net/P_LarT/article/details/107617990](https://blog.csdn.net/P_LarT/article/details/107617990)
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634884312211-0a7f6817-5bb5-4671-b773-4ad0cf39e2cd.png#align=left&display=inline&height=492&id=u2a6aae94&margin=%5Bobject%20Object%5D&name=image.png&originHeight=984&originWidth=2048&size=1516650&status=done&style=none&width=1024)
## Learning Dynamic Routing for Semantic Segmentation 
cvpr20 oral
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634883528108-23c9fafe-d69f-4603-96a9-fa0c05a142ec.png#align=left&display=inline&height=317&id=u374f54f6&margin=%5Bobject%20Object%5D&name=image.png&originHeight=634&originWidth=1856&size=778973&status=done&style=none&width=928)
## Strip Pooling: Rethinking Spatial Pooling for Scene Parsing 
[https://zhuanlan.zhihu.com/p/132532190](https://zhuanlan.zhihu.com/p/132532190)
## Temporally Distributed Networks for Fast Video Semantic Segmentation
[https://zhuanlan.zhihu.com/p/146762297](https://zhuanlan.zhihu.com/p/146762297)
用于快速视频语义分割的时间分布式网络
本文提出了一种时间分布的视频语义分割网络TDNet。从深层CNN的某一高层提取的特征可以通过组合从几个较浅的子网络提取的特征来近似。利用视频中固有的时间连续性，将这些子网络分布在连续帧上。因此，在每个时间步骤中，只需执行轻量级计算即可从单个子网络中提取子特征组。然后应用一种新的注意传播模块来补偿帧间的几何变形，从而重新构造用于分割的全部特征。为了进一步提高全特征层和子特征层的表示能力，还引入了分组知识蒸馏损失。在CityScape、CamVid和NYUD-v2上的实验表明，方法以更快的速度和更低的延迟达到了最先进的精度。
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634873290440-b50891e9-3dfd-4599-9e72-4f57621c6274.png#align=left&display=inline&height=509&id=u868bd204&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1018&originWidth=1068&size=451318&status=done&style=none&width=534)![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634873360150-33d965ff-6a3a-4bae-9be5-7be49b5bc56f.png#align=left&display=inline&height=626&id=uaad70047&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1252&originWidth=2012&size=1080655&status=done&style=none&width=1006)
## SR-Net：[Towards Efficient Scene Understanding via Squeeze Reasoning](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2011.03308)
[https://zhuanlan.zhihu.com/p/355688479](https://zhuanlan.zhihu.com/p/355688479)
[https://blog.csdn.net/weixin_42096202/article/details/109629908](https://blog.csdn.net/weixin_42096202/article/details/109629908)
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634817097134-1b9f8e49-a0ed-43cf-8e7a-c4a957ac6bac.png#align=left&display=inline&height=536&id=uf10e1b0e&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1072&originWidth=1450&size=446258&status=done&style=none&width=725)
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634817121425-c2d05560-e3a5-45fd-92c1-9ed2cbad656f.png#align=left&display=inline&height=255&id=u63103e6a&margin=%5Bobject%20Object%5D&name=image.png&originHeight=510&originWidth=1751&size=465120&status=done&style=none&width=875.5)
## Semantic Flow for Fast and Accurate Scene Parsing
[https://zhuanlan.zhihu.com/p/163864044](https://zhuanlan.zhihu.com/p/163864044)
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634816960732-006ee160-ab2e-44c3-9095-0a4b0419a07c.png#align=left&display=inline&height=456&id=u5dc74ef8&margin=%5Bobject%20Object%5D&name=image.png&originHeight=912&originWidth=1474&size=459714&status=done&style=none&width=737)
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634816982857-1548b6d9-1357-4fc1-a1d8-ccb7acfa3cf4.png#align=left&display=inline&height=150&id=u4281de9d&margin=%5Bobject%20Object%5D&name=image.png&originHeight=299&originWidth=989&size=495423&status=done&style=none&width=494.5)
```python
class AlignModule(nn.Module):
    def __init__(self, inplane, outplane):
        super(AlignModule, self).__init__()
        self.down_h = nn.Conv2d(inplane, outplane, 1, bias=False)
        self.down_l = nn.Conv2d(inplane, outplane, 1, bias=False)
        self.flow_make = nn.Conv2d(outplane * 2, 2, kernel_size=3, padding=1, bias=False)

    def forward(self, x):
        low_feature, h_feature = x  # low_feature 对应分辨率较高的特征图，h_feature即为低分辨率的high-level feature
        h_feature_orign = h_feature
        h, w = low_feature.size()[2:]
        size = (h, w)
        # 将high-level 和 low-level feature分别通过两个1x1卷积进行压缩
        low_feature = self.down_l(low_feature)
        h_feature = self.down_h(h_feature)
        # 将high-level feature进行双线性上采样
        h_feature = F.interpolate(h_feature, size=size, mode="bilinear", align_corners=False)
        # 预测语义流场 === 其实就是输入一个3x3的卷积
        flow = self.flow_make(torch.cat([h_feature, low_feature], 1))
        # 将Flow Field warp 到当前的 high-level feature中
        h_feature = self.flow_warp(h_feature_orign, flow, size=size)

        return h_feature

    @staticmethod
    def flow_warp(inputs, flow, size):
        out_h, out_w = size  # 对应高分辨率的low-level feature的特征图尺寸
        n, c, h, w = inputs.size()  # 对应低分辨率的high-level feature的4个输入维度

        norm = torch.tensor([[[[out_w, out_h]]]]).type_as(inputs).to(inputs.device)
        # 从-1到1等距离生成out_h个点，每一行重复out_w个点，最终生成(out_h, out_w)的像素点
        w = torch.linspace(-1.0, 1.0, out_h).view(-1, 1).repeat(1, out_w)
        # 生成w的转置矩阵
        h = torch.linspace(-1.0, 1.0, out_w).repeat(out_h, 1)
        # 展开后进行合并
        grid = torch.cat((h.unsqueeze(2), w.unsqueeze(2)), 2)
        grid = grid.repeat(n, 1, 1, 1).type_as(inputs).to(inputs.device)
        grid = grid + flow.permute(0, 2, 3, 1) / norm
        # grid指定由input空间维度归一化的采样像素位置，其大部分值应该在[ -1, 1]的范围内
        # 如x=-1,y=-1是input的左上角像素，x=1,y=1是input的右下角像素。
        # 具体可以参考《Spatial Transformer Networks》，下方参考文献[2]
        output = F.grid_sample(inputs, grid)
        return output
```


## SETR：Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspectivewith Transformers
[https://zhuanlan.zhihu.com/p/348418189](https://zhuanlan.zhihu.com/p/348418189)
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1493451/1634816680754-4b5469d5-9e1d-4212-8e88-fc244eb63f02.png#align=left&display=inline&height=241&id=u05ccca87&margin=%5Bobject%20Object%5D&name=image.png&originHeight=481&originWidth=1108&size=399647&status=done&style=none&width=554)
# 最新
[https://www.cityscapes-dataset.com/benchmarks/](https://www.cityscapes-dataset.com/benchmarks/)

